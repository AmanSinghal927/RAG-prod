{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40999e-0b95-408e-92d9-a0658303150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import glob\n",
    "import faiss\n",
    "from fuzzywuzzy import fuzz\n",
    "from llmsherpa.readers import LayoutPDFReader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7a583-acb1-4107-be6e-35813b78e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install sentence-transformers\n",
    "# !pip install openpyxl\n",
    "# !pip install fuzzywuzzy\n",
    "# !pip install llmsherpa\n",
    "# !pip install "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2b380-3866-4138-9f5a-945bb8498db4",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a7c77-587e-438d-ac55-ab7da7a68b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91659119-0abc-42a2-98d4-75a86a967f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    return data[\"raw_text\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855dae87-3a47-4e5d-8e69-e6ec449afb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_from_folders(directory_path):\n",
    "    \"\"\"\n",
    "    Reads all JSON files from each folder in the specified directory.\n",
    "\n",
    "    :param directory_path: Path to the directory containing folders of JSON files.\n",
    "    :return: A list of dictionaries where each dictionary contains data from a single JSON file.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    filenames = []\n",
    "    search_pattern = f\"{directory_path}/*/*.json\"\n",
    "    for file_path in glob.glob(search_pattern):\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            data = split_data(data)\n",
    "            filenames.extend([file_path]*len(data))\n",
    "            all_data.extend(data)\n",
    "    return all_data, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cfc20a-1b02-4e7f-9aa8-d75a06cbd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r'C:\\Users\\J C SINGLA\\Downloads\\External - take_home_challenge_(withJSONs)\\take_home_challenge_(withJSONs)\\raw_text'\n",
    "all_data, filenames = read_json_from_folders(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880cdee-c393-4441-9e15-ff93a97ee3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_name = \"DR--110685614\"\n",
    "subset_str = \"C:\\\\Users\\\\J C SINGLA\\\\Downloads\\\\External - take_home_challenge_(withJSONs)\\\\take_home_challenge_(withJSONs)\\\\raw_text\\\\\" +doc_name+\"\\\\raw_text.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba77b4-d4a9-463d-af8a-09319afc0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(all_data_sherpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480d6af-284f-44f5-8970-f9fe536935b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ace9f-e119-496b-baf8-2f82adee9908",
   "metadata": {},
   "source": [
    "DRAGON+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e0a55-4fc8-41f8-a3d2-79bb5cf01abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\n",
    "query_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\n",
    "context_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\n",
    "\n",
    "# We use msmarco query and passages as an example\n",
    "query =  \"Where was Marie Curie born?\"\n",
    "contexts = [\n",
    "    \"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\",\n",
    "    \"Born in Paris on 15 May 1859, Pierre Curie was the son of Eug√®ne Curie, a doctor of French Catholic origin from Alsace.\"\n",
    "]\n",
    "# Apply tokenizer\n",
    "query_input = tokenizer(query, return_tensors='pt')\n",
    "ctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\n",
    "# Compute embeddings: take the last-layer hidden state of the [CLS] token\n",
    "query_emb = query_encoder(**query_input).last_hidden_state[:, 0, :]\n",
    "ctx_emb = context_encoder(**ctx_input).last_hidden_state[:, 0, :]\n",
    "# Compute similarity scores using dot product\n",
    "score1 = query_emb @ ctx_emb[0]  # 396.5625\n",
    "score2 = query_emb @ ctx_emb[1]  # 393.8340\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d3e08-2468-45db-b207-cc5c1df01474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629436be-810e-4364-b016-04248f1f4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6a51e-f289-4f8d-8c15-0e08845c9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_emb = context_encoder(**ctx_input).last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae5ded-629d-4321-bcdc-b82acb61dc48",
   "metadata": {},
   "source": [
    "# Index Flat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be255b7-530d-4d78-abaa-9ce4235f0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sentence_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(sentence_embeddings)\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9114b-f764-4350-b3aa-618e36052fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # COSINE INDEX\n",
    "# index = faiss.IndexFlatL2(d)\n",
    "# sentence_embeddings = sentence_embeddings / np.linalg.norm(sentence_embeddings, axis=1, keepdims=True)\n",
    "# index.add(sentence_embeddings)\n",
    "# index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ab5a1-8924-42aa-879b-5310e4a4af14",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557643c4-a50b-4ddc-89ad-d6a2f8f729f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path = r\"C:\\Users\\J C SINGLA\\Downloads\\External - take_home_challenge_(withJSONs)\\take_home_challenge_(withJSONs)\\document_questions.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123d726-3b8f-4228-8728-67022a5d2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_excel(ground_truth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24052cde-b1a5-4428-af30-1a2d6280d7f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da3b34e-1d20-4f3f-b6fd-d372d4556a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_text = ground_truth[ground_truth[\"complexity\"]==\"text\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66cf16-5265-410e-9d84-e26102a41a36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ground_truth_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13d8e5-503b-476f-99de-5b715cdcf062",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8cf26-9cc9-4284-9278-aafb07161bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case():\n",
    "    query = [\"What legislative suggestions pertaining to workplace sexual harassment does the firm provide in its submission?\", \n",
    "                  \"What legislative suggestions pertaining to workplace sexual harassment does the firm provide in its submission?\"]\n",
    "    return faiss_inference(query)[0] == faiss_inference(query)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286fa6b-cc2b-4e58-95f4-13b27119d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_inference(idx, data, query, k=4):\n",
    "    if isinstance(query, list):\n",
    "        xq = model.encode(query)\n",
    "        D, I = idx.search(xq, k)\n",
    "        retrieved_items = [[data[i] for i in sublist] for sublist in I]\n",
    "        assert test_case()\n",
    "    else:\n",
    "        xq = model.encode([query])\n",
    "        D, I = idx.search(xq, k)\n",
    "        retrieved_items = [data[i] for i in list(I[0])]\n",
    "    return retrieved_items    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961ba59-be31-4b0c-a5af-1251a761054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = list(ground_truth_text[\"relevant questions\"])\n",
    "test_labels = list(ground_truth_text[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb38ff7-6664-4d01-9b13-942aab90b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_context = []\n",
    "for i in range(len(test_data)):\n",
    "    ret_context.append(faiss_inference(index, all_data_sherpa, test_data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76df83f-1446-4f4f-baab-3dec9b18eace",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a156b1-1653-4c5a-b8d6-f5bab6d43a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_k(test_labels, ret_context, k=len(ret_context[0])):\n",
    "    ctr = 0\n",
    "    correct, incorrect = [], []\n",
    "    for i in range(len(ret_context)):\n",
    "        done = False\n",
    "        if isinstance(test_labels[i], float):\n",
    "            # handle case for when answer is not present in the pdf\n",
    "            continue\n",
    "        for j in range(min(k,len(ret_context))):\n",
    "            if fuzz.partial_ratio(test_labels[i], ret_context[i][j])>=90 and len(ret_context[i][j])>=len(test_labels[i])*0.95:\n",
    "                # print (\"test_labels:\", test_labels[i], \"\\n\", \"ret_context:\", ret_context[i][j])\n",
    "                ctr += 1\n",
    "                done = True\n",
    "                break\n",
    "                \n",
    "        if done == False:\n",
    "            incorrect.append({test_labels[i]:ret_context[i]})\n",
    "\n",
    "        else:\n",
    "            correct.append({test_labels[i]:ret_context[i]})\n",
    "\n",
    "    return ctr/len(ret_context), incorrect, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aad022-8700-4bb1-bdca-aa045f44cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data_sherpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01826fd-fda0-428b-90c8-631eb3a882d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, incorrect, correct = recall_k(test_labels, ret_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a5e58-73cc-4623-bc33-d8cc4de4f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5fd86-f502-43a2-831c-7942c3974cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incorrect[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677e2a5-41c0-4863-b59a-203ab7c5ea39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(incorrect[0].values())[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0926453-031f-44c5-ad76-9136b2897f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"This year we had 52 students from six universities attending the event. Participating schools were: \\nPortland State University \\nUniversity of Idaho \\nUniversity of Portland \\nOregon State University \\nUniversity of Washington \\nOregon Institute of Technology \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e464f29-043e-4a37-9b58-6809e9c3fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string_A = \"This year we had 52 students from six universities attending the event. Participating schools were: \\nPortland State University \\nUniversity of Idaho \\nUniversity of Portland \\nOregon State University \\nUniversity of Washington \\nOregon Institute of Technology\"\n",
    "\n",
    "# string_B = \"This year we had 52 students from six universities attending the event. Participating schools were: \\nUniversity of Idaho \\nUniversity of Portland \\nOregon State University \\nUniversity of Washington \\nOregon Institute of Technology.\"\n",
    "string_A = list(incorrect[0].values())[0][0]\n",
    "string_B = list(incorrect[0].keys())[0]\n",
    "similarity_score = fuzz.partial_ratio(string_A, string_B)\n",
    "similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d80e3-d7f1-412c-9386-2cacc9adf0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sherpa data\n",
    "# missing chunking - 3\n",
    "# not using tokens for search - 2\n",
    "# too many references to fetch - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ca72e-e97b-4a1a-a8aa-1ad6f0267f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data\n",
    "# missing chunking - 3\n",
    "# not using tokens for search - 2\n",
    "# too many references to fetch - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb934b-9c4c-4f94-8b8a-ff94fa712df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c693da-4152-4b1a-86ae-61332034f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = faiss_inference(\"In what cases should online sexual harassment be criminalized according to the submission?\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaff73e-3bd5-4b5a-a3d3-6d17c73b6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc7a0e-6892-477f-8253-0cd83e998d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f77c2430-631f-4375-87c7-ce3bded3c33a",
   "metadata": {},
   "source": [
    "# Better chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4367393-ca58-4fb0-8b8c-c2c98037c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = []\n",
    "# for table in doc.tables():\n",
    "#     tables.append(table.to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a1f99-5797-4fd8-b0de-f6895de168fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(nested_list):\n",
    "    flattened = []\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            flattened.extend(flatten_list(item))\n",
    "        else:\n",
    "            flattened.append(item)\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef19e8-c2f8-4356-ab99-940796523bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "def read_pdf(pdf_url):\n",
    "    pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "    doc = pdf_reader.read_pdf(pdf_url)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cbd2e4-ab3c-4c11-9572-ac0d98e164a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_text(doc, section_title):\n",
    "    \"\"\"\n",
    "    Extracts the text from a specific section in a parsed PDF document.\n",
    "\n",
    "    Parameters:\n",
    "    - doc (Document): A Document object from the llmsherpa.readers.layout_reader library.\n",
    "    - section_title (str): The title of the section to extract.\n",
    "\n",
    "    Returns:\n",
    "    - str: The HTML representation of the section's content, or a message if the section is not found.\n",
    "    \"\"\"\n",
    "    selected_section = None\n",
    "    for section in doc.sections():\n",
    "        if section.title == section_title:\n",
    "            selected_section = section\n",
    "            break\n",
    "\n",
    "    if not selected_section:\n",
    "        return f\"No section titled '{section_title}' found.\"\n",
    "    return selected_section.to_html(include_children=True, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc45928-fcf0-4ad6-b57d-85945bbf44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rechunker(directory_path):\n",
    "    all_data = []\n",
    "    filenames = []\n",
    "    search_pattern = f\"{directory_path}/*.pdf\"\n",
    "    print ()\n",
    "    for i in glob.glob(search_pattern):\n",
    "        print (i)\n",
    "        doc = read_pdf(i)\n",
    "        sections = []\n",
    "        # get all the sections in that doc\n",
    "        for section in doc.sections():\n",
    "            sections.append(section.title)\n",
    "            text = get_section_text(doc, section.title)\n",
    "            all_data.append(text)\n",
    "            filenames.append(i)\n",
    "    return all_data, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ab351-f30e-48f9-9555-93005c236541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_string_lengths(lst1,lst2):\n",
    "    return sum(len(item) for item in lst1 if isinstance(item, str)) + sum(len(item) for item in lst2 if isinstance(item, str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3782c4b-6156-4095-9917-86b69f07e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_criteria(lst):\n",
    "    if lst[-1] == \":\" or \"below\" in \"\\n\".join(lst):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d53bc-19ec-493d-a3bd-5bd65928da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc):\n",
    "    prev_tag = \"\"\n",
    "    doc_sentences = list(doc[\"sentences\"])\n",
    "    doc_tags = list(doc[\"tag\"])\n",
    "    merge_list, j = [], -1\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        # print (i, prev_tag, doc_tags[i])\n",
    "        if doc_tags[i] == \"para\" and prev_tag == \"para\" and check_criteria(merge_list[j]):\n",
    "            merge_list[j].extend(doc_sentences[i])\n",
    "            prev_tag = \"para\"\n",
    "            \n",
    "        elif doc_tags[i] == \"list_item\" and prev_tag == \"para\":\n",
    "            merge_list[j].extend(doc_sentences[i])\n",
    "            prev_tag = \"list_item\"\n",
    "            \n",
    "        elif doc_tags[i] == \"list_item\" and prev_tag == \"list_item\":\n",
    "            merge_list[j].extend(doc_sentences[i])\n",
    "            prev_tag = \"list_item\"\n",
    "            \n",
    "        elif doc_tags[i]==\"para\":\n",
    "            merge_list.append(doc_sentences[i])\n",
    "            prev_tag = \"para\"\n",
    "            j = j + 1\n",
    "        else:\n",
    "            prev_tag = doc_tags[i]\n",
    "    return merge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0d8d5-376a-4c2f-85fe-bbc15339a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    doc = [x.encode('ascii', 'ignore').decode('ascii') for x in doc if len(x)>100]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bd43d-bb4b-42e3-bbf8-4c2b30939910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKE: llmsherpa messing up headers but paras are good\n",
    "# still some paras might be messed up e.g. very small paras that are actually numbers\n",
    "def get_paras(directory_path):\n",
    "    all_data = []\n",
    "    filenames = []\n",
    "    search_pattern = f\"{directory_path}/*.pdf\"\n",
    "    for i in glob.glob(search_pattern):\n",
    "        print (i)\n",
    "        doc = read_pdf(i)\n",
    "        doc = pd.DataFrame(doc.json)\n",
    "        doc = process_doc(doc)\n",
    "        doc = [\"\\n\".join(x) for x in doc]\n",
    "        doc = clean_doc(doc)\n",
    "        filenames.extend([i]*len(doc))\n",
    "        all_data.append(doc)\n",
    "        print (len(all_data), len(filenames))\n",
    "    return all_data, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e097198-7e34-4caf-9019-c2a1b750bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_list_pairs(list1, list2):\n",
    "    unique_list1 = set()\n",
    "    unique_list2 = []\n",
    "    for i, item in enumerate(list1):\n",
    "        if item not in unique_list1:\n",
    "            unique_list1.add(item)\n",
    "            unique_list2.append(list2[i])\n",
    "    return list(unique_list1), unique_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ff85a-16a6-4a61-baed-29106715a711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data_sherpa, filenames_sherpa = get_paras(\"/Users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f1d66-8b86-4e49-9e03-e2c3d730507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_sherpa = flatten_list(all_data_sherpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdf188-4fc4-47de-8861-179dab892155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(all_data_sherpa), len(filenames_sherpa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7f9fc-75ae-473b-84b0-7efae8f4abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_sherpa[63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76723096-4dfc-4c45-9367-531a2c25b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r\"C:\\Users\\J C SINGLA\\Downloads\\External - take_home_challenge_(withJSONs)\\take_home_challenge_(withJSONs)\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6732d3c-c716-43c9-a184-e9bc6014aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(directory, filename, lst):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(lst, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004ec35-a2ed-47ee-8dab-bee02bf755f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_from_file(directory, filename):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return None  # or an empty list, depending on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9e8c5-6346-4dc5-837c-d9e19199a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_sherpa = read_list_from_file(save_path, \"sherpa_paras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dfa3b5-b690-4f02-aa2e-088fc33e1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_sherpa = read_list_from_file(save_path, \"sherpa_paras_filenames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a798d-b274-4708-a445-4b7b3b7ea937",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filenames_sherpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a6682-3fd6-4772-b82e-797ae07bcbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_list_to_file(save_path, \"sherpa_paras_filenames\", filenames_sherpa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5750c9-2bbd-4e76-8bb3-b74b3d231404",
   "metadata": {},
   "source": [
    "# Create an index on completed chunks to map to incomplete chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a87b5-021e-4005-8e00-1a2c9a34c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_sherpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d049d753-2462-40c1-a5a0-f6612d24b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings1 = model.encode(all_data_sherpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad350ca-1e26-43a7-aa41-c4993ae793e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sentence_embeddings.shape[1]\n",
    "index1 = faiss.IndexFlatL2(d)\n",
    "index1.add(sentence_embeddings)\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831f173-3c78-42e3-9f7d-5e9acd2737eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ret_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c834f2-fd54-4f93-a786-7a3908a3b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rechunk_ret_context = []\n",
    "for i in range(len(ret_context)):\n",
    "    temp = []\n",
    "    for j in range(len(ret_context[i])):\n",
    "        temp.extend(faiss_inference(index1, all_data, ret_context[i][j], 1))\n",
    "    rechunk_ret_context.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd60080-4f76-43ed-b0b7-f9b27140d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "rechunk_ret_context[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ac000-11a1-4beb-ba89-a678784e5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_context[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069fcf0-a5e5-41b8-beca-c798907893de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fcb7c-8e3d-4301-ad2c-ed210ba692e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace8ad8-98ea-494b-979f-b1c4caf7a3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6819a1-bc1a-4115-b27f-25ed819064d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data + all_data_sherpa\n",
    "filenames = filenames + filenames_sherpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319c611-8fe9-4856-a388-5fdbf73ce25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: we need better cleaning on this\n",
    "all_data_, filenames_ = dedup_list_pairs(all_data, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff78882-2c83-4a47-8bf4-000290406dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7da5cc-f037-4e4e-ac25-8ec1fa43bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEAWAY: some of these are headers and we want to remove them\n",
    "# Assumption: The distribution of lengths will be bimodal -> True/bimodal with peak at 25 characters -> let's start removing with 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64aa37-b495-44c1-b2bb-28cd37c27f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = [len(sentence) for sentence in all_data]\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(word_counts, bins=500, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(range(min(word_counts), max(word_counts) + 1, 25))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621b4ee-0b64-45e1-a7d0-fc0d13fccd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takeaway: manually inspected the values less than length 100\n",
    "all_data = [(k,v) for (k, v) in zip(all_data_, filenames_) if len(k)>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928a3d8-d4b1-42c1-b510-0b91dc12c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [i[1] for i in all_data]\n",
    "all_data = [i[0] for i in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c808b-33ac-4e21-9532-b81b033efb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd86c6a-bde0-4e13-bfec-3d34eb8bf25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"\"\"\n",
    "The Student Liaison Committee had another successful year in attracting schools from the Pacific \n",
    "Northwest to compete in the Oregon Section's annual Traffic Bowl Competition. The Traffic Bowl is \n",
    "a Jeopardy-based trivia contest where students must answer questions on a variety of traffic and \n",
    "transportation trivia. This year the competition was held on November 15, 2007 at McMenamin's \n",
    "Edgefield just east of Portland, Oregon.  \n",
    "This year we had 52 students from six universities attending the event. Participating schools were: \n",
    "Portland State University \n",
    "University of Idaho \n",
    "University of Portland \n",
    "Oregon State University \n",
    "University of Washington \n",
    "Oregon Institute of Technology \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84864719-1ea3-45e1-ab1b-b40f66305d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c3983e-2d41-4988-af92-d8efcd2226d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02805f3d-47dd-4fd6-bb1e-c9766dc193f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
